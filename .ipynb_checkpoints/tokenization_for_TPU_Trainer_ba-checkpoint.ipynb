{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmvmfuxDgaUF"
   },
   "source": [
    "# Tokenization for Huggingface Transformer model (GPT2/Reformer/TransformerXL/...) training on HUWIKI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hz0k0OxBj8DN"
   },
   "source": [
    "## Get data\n",
    "<br>\n",
    "<font size=\"4\"> \n",
    "We have a preprocessed huwiki dump (20200520) stored as a list of txt files in a google cloud bucket at <br> \n",
    "<code>gs://hungpt2-wikipedia/full_wiki_cleaned/*</code> <br><br>\n",
    "Here we access it from local storage when neeed <br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4xW6_u2fydp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sourceDir = '/home/adamb/Documents/huwiki_preprocessed/'\n",
    "print(os.listdir(sourceDir+'/*wiki*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "BdIFeFBKwY5k",
    "outputId": "75033b19-2f8c-411e-a1d4-97efa57b4239"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5488b5d9dc36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# authorize access to bucket from colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# authorize access to bucket from colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kqfn0uhEgqmV"
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "#### WikiExtractor.py (https://github.com/attardi/wikiextractor) is a Python script that extracts and cleans text from a Wikipedia database dump. It stores output in a number of files of similar size in a given directory. <br> Each file will contain several documents in the format:\n",
    ">\\<doc id=\" \" revid=\" \" url=\"\" title=\" \"\\>\n",
    "><br>...<br>\n",
    ">\\</doc\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gJEIp6SJgeu0",
    "outputId": "5ab790ee-6f53-42aa-e3cb-e523f6b8b672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wikiextractor' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# install from git\n",
    "!git clone https://github.com/attardi/wikiextractor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "6eY285h3gre8",
    "outputId": "ea4d87af-9b03-4b3d-f46f-db0b19a26140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘full_wiki_extract’: File exists\n",
      "XML dump files to process:\n",
      "['/content/hunwiki/huwiki-20200520-pages-articles-multistream3.xml-p198204p406074.bz2', '/content/hunwiki/huwiki-20200520-pages-articles-multistream6.xml-p1116439p1705558.bz2', '/content/hunwiki/huwiki-20200520-pages-articles-multistream2.xml-p58602p198203.bz2', '/content/hunwiki/huwiki-20200520-pages-articles-multistream4.xml-p406075p692318.bz2', '/content/hunwiki/huwiki-20200520-pages-articles-multistream1.xml-p1p58601.bz2', '/content/hunwiki/huwiki-20200520-pages-articles-multistream5.xml-p692319p1116438.bz2']\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml0’: File exists\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml1’: File exists\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml2’: File exists\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml3’: File exists\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml4’: File exists\n",
      "mkdir: cannot create directory ‘/content/full_wiki_extract/xml5’: File exists\n",
      "Output dirs for preprocessing:\n",
      "['/content/full_wiki_extract/xml0', '/content/full_wiki_extract/xml1', '/content/full_wiki_extract/xml2', '/content/full_wiki_extract/xml3', '/content/full_wiki_extract/xml4', '/content/full_wiki_extract/xml5']\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream3.xml-p198204p406074.bz2\n",
      "Output dir is /content/full_wiki_extract/xml0\n",
      "WARNING: Template errors in article 'Ptolemaiosz-tétel' (320003): title(4) recursion(0, 0, 0)\n",
      "Elapsed time 289.18610978126526\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream6.xml-p1116439p1705558.bz2\n",
      "Output dir is /content/full_wiki_extract/xml1\n",
      "Elapsed time 740.3403475284576\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream2.xml-p58602p198203.bz2\n",
      "Output dir is /content/full_wiki_extract/xml2\n",
      "WARNING: Template errors in article 'Alakváltozás' (178070): title(1) recursion(0, 0, 0)\n",
      "Elapsed time 280.3689479827881\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream4.xml-p406075p692318.bz2\n",
      "Output dir is /content/full_wiki_extract/xml3\n",
      "WARNING: Template errors in article 'Temesy Győző' (433326): title(1) recursion(0, 0, 0)\n",
      "WARNING: Template errors in article 'Enantiomerfelesleg' (526861): title(1) recursion(0, 0, 0)\n",
      "Elapsed time 327.4344913959503\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream1.xml-p1p58601.bz2\n",
      "Output dir is /content/full_wiki_extract/xml4\n",
      "WARNING: Template errors in article 'Tiszalök' (45351): title(1) recursion(0, 0, 0)\n",
      "Elapsed time 242.25930833816528\n",
      "Processing /content/hunwiki/huwiki-20200520-pages-articles-multistream5.xml-p692319p1116438.bz2\n",
      "Output dir is /content/full_wiki_extract/xml5\n",
      "WARNING: Template errors in article '304 (szám)' (967752): title(1) recursion(0, 0, 0)\n",
      "WARNING: Template errors in article 'Gravitációs tér' (978489): title(1) recursion(0, 0, 0)\n",
      "Elapsed time 538.2397539615631\n"
     ]
    }
   ],
   "source": [
    "# create target dir\n",
    "!mkdir full_wiki_extract\n",
    "\n",
    "import time \n",
    "import os\n",
    "import glob\n",
    "\n",
    "# list xml dump files\n",
    "dumpFiles = glob.glob('/content/hunwiki/*xml*bz2')\n",
    "print('XML dump files to process:')\n",
    "print(dumpFiles)\n",
    "# create dirs for the output from each dump\n",
    "outputDirs = []\n",
    "for i in range(len(dumpFiles)):\n",
    "  outputDirs.append('/content/full_wiki_extract/xml'+str(i))\n",
    "  os.environ['SUBDIR'] = outputDirs[i]\n",
    "  !mkdir $SUBDIR\n",
    "print('Output dirs for preprocessing:')\n",
    "print(outputDirs)\n",
    "\n",
    "# process each dump file, save outputs to separate dir ,measure elapsed time:\n",
    "for idx, dumpFile in enumerate(dumpFiles):\n",
    "  print('Processing ' + dumpFile)\n",
    "  print('Output dir is ' + outputDirs[idx])\n",
    "  start = time.time()\n",
    "  # we pass input name and output dir as env vars to the wikiextractor script\n",
    "  os.environ['DUMPFILE'] = dumpFile\n",
    "  os.environ['OUTPUTDIR'] = outputDirs[idx]\n",
    "  # invoke wikiextractor script\n",
    "  !python wikiextractor/WikiExtractor.py $DUMPFILE --processes 4 --bytes=25M  --filter_disambig_pages --output=$OUTPUTDIR --min_text_length 100 -q\n",
    "  end = time.time()\n",
    "  print(f'Elapsed time {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjy4IEAeg9zr"
   },
   "source": [
    "<br></br>\n",
    "#### Collect all txt files under /content/full_wiki_extract, append their names with the no. of the source xml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVoaKYHooAue"
   },
   "outputs": [],
   "source": [
    "# rename files according to their origin xml + move them under /content/full_wiki_extract/\n",
    "for idx, outputDir in enumerate(outputDirs):\n",
    "  # file names\n",
    "  fileNames = os.listdir(outputDir+'/AA')\n",
    "  # xml number from dumFiles list\n",
    "  xmlNo = os.path.split(dumpFiles[idx])[1].split('.')[0][-1]  # last digit before the \"\".xml\" part in the filename\n",
    "  # new file names\n",
    "  newFileNames = ['xml'+str(xmlNo)+'_'+f for f in fileNames]\n",
    "  # new paths\n",
    "  newPaths = ['/content/full_wiki_extract/'+newName for newName in newFileNames]\n",
    "  # move files\n",
    "  for fileIdx in range(len(fileNames)):\n",
    "    os.rename(outputDir+'/AA/'+fileNames[fileIdx], newPaths[fileIdx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeQmxvMqWh1w"
   },
   "source": [
    "#### Check the results and save them out to a bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uaghiiEnR3BF",
    "outputId": "3e05105a-4a9c-4983-c7d8-c18b052f0943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xml0', 'xml3', 'xml4_wiki_00', 'AA', 'xml2', 'xml1', 'xml3_wiki_00', 'xml2_wiki_00', 'xml5_wiki_00', 'xml4', 'xml1_wiki_00', 'xml5']\n"
     ]
    }
   ],
   "source": [
    "# let's see what we have\n",
    "print(os.listdir('/content/full_wiki_extract'))\n",
    "\n",
    "# copy all txt files to google cloud bucket\n",
    "!gsutil cp /content/full_wiki_extract/*wiki* gs://hungpt2-wikipedia/full_wiki_extract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYweKSGESqtu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpP8GIWsUohh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tHmKICPUq03"
   },
   "source": [
    "Create one article per line .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yCCMoZWg9WN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "test_wikis = glob('/content/test/*/*')\n",
    "\n",
    "for i,file in enumerate(test_wikis):\n",
    "  with open(file) as f:\n",
    "    wikitext = f.read()\n",
    "  wikitext = wikitext.split('</doc>')\n",
    "  wikitext = [' '.join(text.split('\\n')[3:]) for text in wikitext]\n",
    "  wikitext = '\\n'.join(wikitext)\n",
    "  filename = os.getcwd()+'/preprocessed_wiki/'+'test/'+str(i)+'.txt'\n",
    "  os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "  with open(filename,'w') as f:\n",
    "    f.write(wikitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "UCVM5089aa-S",
    "outputId": "19311251-b584-4781-958c-90c32d4464d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sárrétudvari nagyközség Hajdú-Bihar megyében, a Püspökladányi járásban.  Hajdú-Bihar megyében, Biharnagybajomtól északnyugatra, a Nagy-Sárrét központi részén fekvő település.   A régészeti leletek arról tanúskodnak, hogy a községünk területén már i. e. 5000 évvel is éltek emberek. Itt hagyták jellegzetes temetkezési helyeiket a rézkor végén (i. e. 2000) élő pásztornépek is. Ezeket mi kunhalmoknak és kurgán-oknak nevezzük. Pl: a Tikicséri halom Sárrétudvari és Biharnagybajom között. A korai vaskorból (i. e. 500) a szkíták, majd a kelták és az őket követő szarmaták is nyomot hagytak maguk után.  Az i. sz. 4. században gepidák lakták ezt a helyet, akiket 560 körül legyőztek az avarok. A 9. században megjelentek a honfoglaló magyarok. A régészti ásatások a Hízóföldön arról tanúskodnak, hogy \"vagyonos, katonáskodó, fegyverrel bőven felszerelt, asszonyaikat nemesfém ékszerekkel gazdagon ellátó, rangos középréteg, a honfoglalók első nemzedéke telepedett itt le, magukba olvasztva az itt talált gyér szláv és besenyő lakosságot.\" – írta Nepper Ibolya régész.  Udvari régi templomának nyomai alapján feltételezhető, hogy már a 11. század második felében állt. A sírokban pogány és keresztény szokás meglétét fedezték fel. Ebben az időszakban kerülhetett a falu királyi adományként a Zovárd nemzetség birtokába. Ekkor már valószínű az \"Udvari\" nevet viselte a település. A Váradi Regestrum 1214-ben megemlíti Udvarit.  Az 1514-es Dózsa felkeléshez az udvariak is csatlakoztak, és együtt dúlták fel a bajomi parasztokkal a Bajoni család udvarházát, égették el irataikat. A Perényi Ferenc váradi püspök által összehívott nemesek Várad alatt verték le őket.  A török magyarországi terjeszkedésének következtében 1555-ben Udvari 47 portával török uralom alá került, majd 1556-ban az erdélyi fejedelmi kincstár rendelkezett felette, az un. Partiumhoz (Részekhez) tartozott. Ez azt jelentette, hogy mindhárom magyarországi főhatalomnak (királyi, fejedelmi és török uralom) kiszolgáltatottjává vált. Udvari éppen erre a hármas érintkezési pontra esett, így aztán mindenhonnan sarcolták, és vitték el adóját. A környék lakosságát a Sárrét mocsárvilága sokszor megmentette ellenségeitől.  1607-ben Udvarit Báthory Gábor erdélyi fejedelem Nagy Andrásnak adományozta, majd 1613-tól Kamuti Balázsnak, aki a templomot fedeleztette. 1656-tól Ebeni István, 1665-től Béldi Pál, 1692-től Wesselényi Pál birtokolta Udvarit.1694-től ismét a váradi püspök birtokolta, dézsmálta a falunkat.  A Rákóczi-szabadságharc idején élt és jelentős birtokai voltak Udvariban is Bodó Jánosnak, akinek leányát Bodó Katát vette feleségül Nyíri András, a kuruc hadak ezredes kapitánya. (A szabadságharcban meghalt, s a templomkertben temették el a holttestét.)  1715-től Udvari (53 családfővel) ismét a váradi püspökség birtokába került. Az 1700-as évek második felétől Bihar vármegye része lett a település. 1740-ben járvány következtében a lakosság 90%-a elpusztult.  A falu 1785-ben még kétutcás, szalagtelkes település volt.  Az 1848–49-es forradalom és szabadságharcban udvari önkéntesek is harcoltak. Az elesett honvédek sírjait az új temetőben találjuk. Az udvari emberek nemzeti érzelmeiről árulkodik az a feljegyzés, hogy 1892. szeptember 18-án Kossuth Lajost a 90. születésnapján az udvari elöljárók díszpolgárrá avatták. Kossuth temetésén küldöttséggel képviseltette magát a község, koszorút helyeztek el a ravatalán. Itthon pedig gyászistentiszteletet tartottak. 1893-ban Jókai Mórt írói munkássága 50. évfordulója alkalmából díszpolgárrá választották és megrendelték műveinek gyűjteményes kiadását a Népkönyvtár számára.  Magyarország ezeréves fennállásának emlékére 1896. május 9-én a községházán díszgyűlést rendeztek. Kovács Károly helybéli kovácsmester mozsárágyút öntetett saját pénzén a millennium emlékére.  Az elöljárók törődtek a falu fejlődésével. 1850-ben az utak kavicsolását jegyezték le.  1910-ben már 4 kövesút indul Udvariból a szomszédos helységek felé. Az 1898-ban megnyílt Püspökladány-Szeghalom szárnyvonalon át bekerült Sárrétudvari a vasútforgalomba.  1720-ban földesúri tulajdonban Udvari lakosságát 3 szárazmalom látta el liszttel.  A 19-20. század fordulóján gőzhengermalmot építtetett a község.  1932-ben Göttler Nándor pékmester péküzemet alapított.  1801-ben említik a bábát, majd 1873-ban orvosi állomás létesítéséről, 1876-ban okleveles szülésznő alkalmazásának elrendeléséről intézkedtek az elöljárók.  1880-ban orvosi lakást vásároltak. Ez idő tájt létesült gyógyszertár is a falunkban Szekér Lajos füzesgyarmati gyógyszerész tulajdonában. 1878-ban alakult meg a helybéli Tűzoltó Egyesület. 1893-ban egyesítették az olvasóköröket és a községi könyvtárat, hogy a Község Népkönyvtára kibővüljön.   2001-ben a település lakosságának 95%-a magyar, 5%-a cigány nemzetiségűnek vallotta magát.  Az iskolai oktatásról a 17. század elejéről vannak adataink. Akkor még nem volt főhivatású tanítójuk, a tanítást a prédikátor látta el. Az első fennmaradt rektori díjlevelet 1760-ban jegyezték fel. 1809-ben 6 fiúosztályt sorolnak fel. 1863-ban egy-egy földtekét vásároltak a fiú- és leányiskolának.  A tanítók jövedelmezését az egyház szolgáltatta. A gyermekek iskolába való felíratását a presbiterek szorgalmazták, ellenőrizték, élükön a prédikátorral és a kurátorral. A tanítók kiválasztása ugyancsak az ő hatáskörükbe tartozott. Az iskolák fűtése az egyház gondjaira volt bízva.  A 20. század elején egyre nagyobb terhet jelentett a Sárrétudvari Református Egyháznak az iskolák fenntartása.  1910-ben 3 tantermes iskola építését kezdték el. A sárrétudvari hat elemi és három ismétlő iskola 1948-ig maradt a református egyház kezelésében.  A két világháború közötti időszakban öt épület kilenc tantermében folyt az oktatás.  A hétköznapokról a község költője, Nagy Imre így ír: <poem> Tízen is köhögnek egyszerre, s egész éjszaka hangos a ház. A belopódzó holdsugáron A halál csontkeze citeráz. Kinn vakit a téli fehérség, bent a halál épit temetőt, sarkokból a pók fagyva lehull, s nem bírja szőni a szemfedőt. Reggel a falon véres dér lesz, s véres az ablak jégvirága, - tavaszra a kis sárviskóból kripta lesz – holtak zord világa.  (Tízen is)</poem>  A tanév végén a templomban voltak a nagy vizsgák. Az egész falu népét hívta a lelkész, hogy színük előtt adjanak számot a gyermekek a tanultakról. A hatodik osztály végén ágendáztak, utána felvették az úrvacsorát.  Napjainkban három épületben folyik a tanítás és 1995. január 1-je óta Általános és Alapfokú Művészeti Iskola lett.   Az eklézsia rövid története:  Egész bizonyossággal nem állapítható meg, melyik esztendőben, miképpen és kik által kezdődött meg a településen a reformáció. Semmi olyan írás vagy jegyzet, mely erre fényt derítene nem található az egyház levéltárában.  A település régi neve Biharudvari, amely régen tekintélyes püspöki birtok, és a 13. században 61 lakosú hely volt. Fráter György püspök el akarta cserélni, ez azonban nem sikerült, mert 1552-ben is a püspökséghez tartozott. A 15. században az Izsákai és a Bessenyei családoknak is volt ott birtokrészük. A református templomról a műemlékké nyilvánítási szakvéleményben ezt olvassuk : „A jelenlegi templom helyén épült a falu első temploma 1613-ban Egri György bíró és Thuri Gábor prédikátorsága alatt. A falu növekedésével szűknek bizonyult, így 1764 és 1769 között Segesvári János lelkipásztor szolgálata alatt épült a mai templom Mulich Bertalan mérnök tervei alapján. A torony húsz évvel később 1788-89-ben készült el. A templom belső tere 9 x 23 m, 800 ülőhelyes berendezése 1871-ben készült el. Ekkor építette Országh Sándor az orgonát, mely a háborúban tönkrement. Orgona szekrénye oldalán felirat olvasható: készítette: Nagy Madar Sándor 1871-ben. 1882-ben a torony felső része leég, melyet közadakozásból újjáépítenek, az adakozó tehetős, gazdag emberek nevét megörökítik a gerendákon, 1926-ban új csillagot és gombot helyeznek a toronycsúcsra.\"  A templom:  „A templom a község szélén épült kelet-nyugati tengellyel. A nyugati homlokzat előtti tornya kifelé néz a községből. A tornyot a templom gerince fölött egy többtagú osztó párkány két részre osztja, így az alsó két emelet magassága, falsávos és tükrös. A felső szintje pedig pilaszterekkel keretezett órapárkányos. A toronyház ablakai könyöklő párkányosak. A templom fala is falsávos és tükrös tagolással készült. A déli oldalán nagy, a keletin és az északin rövidebb félkör ívesen záródó ablakokkal. A déli térben sík, vakolt mennyezet, három oldalon körbefutó karzat van benne.”  A templomi rend:  „Sárrétudvari keleti-nyugati tájolású templomának belső terét középen a piac osztja ketté. Innen az épület hosszanti tengelyében széles utak vezetnek a végfalakon levő ajtókig, a kettéosztott padoszlopok között. A középső tér hosszanti falán van a szószék, alatta a papi székkel. Vele szemben a keresztpad kettéosztott oszlopsora van. A piac közepén az épület centrumában az Úr asztala áll. 1882-ben alacsony kerítést készítettek az Úr asztal aköré. Az asztal közepén a Bibliát helyezték el. Tavasztól őszig virág illatozott a Biblia mellett, ősztől tavaszig búzacsokor került a virág helyébe. A templom belső falát három oldalról karzat övezi. A hosszanti fal közepén az orgona áll, mellette és a végkarzaton padoszlopok vannak. (Sokkal egyszerűbbek a földszintieknél.) a karzat szintjét a fal irányában megemelték, így mindenki ráláthat a szószékre.” Az első világháború előtt hímes, szőttes terítő volt az Úr asztalán, utána bordó bársony került a helyére. A szószék mellvédjét és lépcsőjének korlátját, valamint a középső teret övező kerítés vízszintes lapját ugyanolyan terítőkkel látták el. A bordó mellett fekete színű garnitúrát is készítettek. Évente háromszor gyászba öltöztették a templomot, Nagypénteken, október 6-án és Szilveszterkor. Ezenkívül a templomban tartott temetések alkalmával is használatba kerültek a gyászterítők.  Itt születtek:  Itt éltek:    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = os.getcwd()+'/preprocessed_wiki/'+'test/'+str(1)+'.txt'\n",
    "with open(filename) as f:\n",
    "  print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3O_dwBmGwfQu"
   },
   "source": [
    "**Train BPE tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "fvGOooYOwipE",
    "outputId": "0f698324-4259-482c-f720-52583e15a3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 3.0MB 13.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1MB 51.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 52.3MB/s \n",
      "\u001b[?25h  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "import transformers\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "XGUz5D2Aw03O",
    "outputId": "29b7eb83-c9a9-48c3-82cd-9488c1febae8"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-42ceea977f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mOUT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hunwiki_tokenizer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hunwiki\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, pretty)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0mTokenizer\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths = glob(\"/content/preprocessed_wiki/*/*\")\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8000, min_frequency=3, special_tokens=[\"<|endoftext|>\"]) #\"[PAD]\",\n",
    "\n",
    "# Save tokenizer\n",
    "OUT_DIR = \"hunwiki_tokenizer\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "tokenizer.save(OUT_DIR+\"/hunwiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_kXuS2hV411"
   },
   "outputs": [],
   "source": [
    "tokenizer.save(OUT_DIR+\"/hunwiki_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R8FMLu34V-IE",
    "outputId": "4351bbff-211c-4fc7-aeb0-e6b8adf37622"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-qL_xyi_cQU"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzDz17YzRQ8d"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hAL2EWupVnLv",
    "outputId": "bff79a2a-788e-4724-fab6-2dc54a5310da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/hunwiki_tokenizer/hunwiki_tokenizer [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 1 objects/209.1 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/hunwiki_tokenizer/hunwiki_tokenizer gs://hungpt2-wikipedia/hunwiki_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "da3FiUdeZXLZ",
    "outputId": "9ec47cc9-ba27-4203-a473-62c17cc68af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/preprocessed_wiki/test/0.txt [Content-Type=text/plain]...\n",
      "Copying file:///content/preprocessed_wiki/test/1.txt [Content-Type=text/plain]...\n",
      "Copying file:///content/preprocessed_wiki/test/2.txt [Content-Type=text/plain]...\n",
      "Copying file:///content/preprocessed_wiki/test/3.txt [Content-Type=text/plain]...\n",
      "/\n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///content/preprocessed_wiki/test/4.txt [Content-Type=text/plain]...\n",
      "-\n",
      "Operation completed over 5 objects/101.5 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/preprocessed_wiki/test/*.txt gs://hungpt2-wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZ9uniTysbxI"
   },
   "source": [
    "Create tokenized files with uniform length samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qUHceYhVfHU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2NcMtILsbZ2"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "preopcessed_wikis = glob('/content/preprocessed_wiki/test/*.txt')\n",
    "examples = []\n",
    "for i,file in enumerate(preopcessed_wikis):\n",
    "  with open(file) as f:\n",
    "    prep_wiki = f.read()\n",
    "  ids = tokenizer.encode(prep_wiki).ids\n",
    "  for i in range(0, len(ids) - MAX_LEN + 1, MAX_LEN):  # Truncate in block of MAX_LEN\n",
    "    examples.append(ids[i : i + MAX_LEN])\n",
    "\n",
    "with open('train_examples.txt','w') as f:\n",
    "  f.write('\\n'.join(str(example) for example in examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SzRO3UNKcQm3",
    "outputId": "196488e6-76b3-445a-88ce-2f5005fd1876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/train_examples.txt [Content-Type=text/plain]...\n",
      "|\n",
      "Operation completed over 1 objects/134.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /content/train_examples.txt gs://hungpt2-wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhQqMXHT0Jex"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "examples_array = np.array(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ymM1U7ThX8AL",
    "outputId": "393604d2-16f5-46bf-9d78-451381df5dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./preprocessed_wiki/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBiBCowHaLgr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9w_6AD_E2FJ"
   },
   "source": [
    "# Trainer class approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "YFN5I_4SFNzJ",
    "outputId": "cc88b151-2cd2-4ddb-844d-14b211aaef6e"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-17ae39acafc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgpt_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/hunwiki_tokenizer/hunwiki-vocab.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/hunwiki_tokenizer/hunwiki-merges.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/tokenization_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, merges_file, unk_token, bos_token, eos_token, add_prefix_space, trim_offsets, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mmerges_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerges_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_prefix_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0mtrim_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_offsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             ),\n\u001b[1;32m    346\u001b[0m             \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/byte_level_bpe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, merges_file, add_prefix_space, lowercase, dropout, unicode_normalizer, continuing_subword_prefix, end_of_word_suffix, trim_offsets)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mcontinuing_subword_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinuing_subword_prefix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mend_of_word_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_of_word_suffix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 )\n\u001b[1;32m     36\u001b[0m             )\n",
      "\u001b[0;31mException\u001b[0m: Error while initializing BPE: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "config = GPT2Config(n_positions=256,\n",
    "                    n_ctx=256,\n",
    "                    vocab_size=8000,\n",
    "                    bos_token_id=0,\n",
    "                    eos_token_id=0)\n",
    "\n",
    "model = TFGPT2LMHeadModel(config=config)\n",
    "gpt_tokenizer = GPT2TokenizerFast('/content/hunwiki_tokenizer/hunwiki-vocab.json','/content/hunwiki_tokenizer/hunwiki-merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8A1_yNNq-LuW"
   },
   "outputs": [],
   "source": [
    "gpt_tokenizer.pad_token='<|endoftext|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTdsiNLYMcH3"
   },
   "source": [
    "**Build tf.Dataset**\n",
    "\n",
    "To check what is the valid input format(features, labels) for the model, load a pretrained GPT2 and do succesful a forward pass, that returns loss, and logits.\n",
    "We want to replicate the behavior in \"_run_model\" method of TFTrainer class.\n",
    "\n",
    "What we need as input are fix length tokenized chunkes as inputs and labels=inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6m82s_LHAMt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDcCu-Ti0H7R"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(examples_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vsIA2FbvOXIH"
   },
   "outputs": [],
   "source": [
    "from transformers import TFTrainer, TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"/content/hunGPT2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8\n",
    ")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekww9yV28YLH"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-7cRZre6OreQ",
    "outputId": "565980f0-23de-4fd6-e516-4c8cf1faf17b"
   },
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36m_training_steps\u001b[0;34m(self, ds, optimizer)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulate_next_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36m_accumulate_next_gradients\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0m_accumulate_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/transformers/trainer_tf.py:343 _accumulate_next  *\n        per_replica_features, per_replica_labels = next(iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:561 __iter__\n        self._disallow_iteration()\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:554 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\n        \" decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Awy-EtdkzZ2K"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TextLineDataset('/content/train_examples.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHpFXD0BYCnZ"
   },
   "outputs": [],
   "source": [
    "from transformers import TextDataset\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    file_path=\"/content/preprocessed_wiki/test/0.txt\",\n",
    "    block_size=config.n_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDBBeQCPgFU_"
   },
   "source": [
    "Load hunwiki pretrained tokenizer vocab to tf tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QO2aI_v5gE7h",
    "outputId": "6a950078-db69-4b5d-ee87-adb989cc92b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/content/hunwiki_tokenizer/hunwiki-vocab.json') as json_file:\n",
    "  vocab = json.load(json_file)\n",
    "\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Joet29Mg5mk"
   },
   "outputs": [],
   "source": [
    "vocab_words = vocab.keys()\n",
    "\n",
    "with open('vocab.txt','w') as f:\n",
    "  f.write(\"\\n\".join(list(vocab_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NR0WxchRHaUU"
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(example):\n",
    "  return gpt_tokenizer.encode(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SdIeLOPGxtW"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "text_paths = glob('/content/preprocessed_wiki/test/*.txt')\n",
    "\n",
    "train_dataset = tf.data.TextLineDataset(text_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4dR4Fh31Ziz"
   },
   "outputs": [],
   "source": [
    "gpt_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7tdgWRQpqPC"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def encode_ds(example):\n",
    "  tokens = gpt_tokenizer.encode(example.numpy().decode('utf-8'),pad_to_max_length=True,max_length=256)\n",
    "  # tf.keras.preprocessing.sequence.pad_sequences(tokens,padding='post',maxlen=MAX_LEN)\n",
    "  # tokens= tokens[:MAX_LEN]\n",
    "  # for i in range(0, len(tokens) - max_len + 1, max_len):  # Truncate in block of block_size\n",
    "  #   examples.append(tokens[i : i + max_len])\n",
    "  return [tokens]\n",
    "\n",
    "def encode_ds_map_fn(example):\n",
    "  # py_func doesn't set the shape of the returned tensors.\n",
    "  encoded_text = tf.py_function(encode_ds, \n",
    "                                inp=[example], \n",
    "                                Tout=(tf.int32))\n",
    "  encoded_text.set_shape(MAX_LEN)\n",
    "\n",
    "  return encoded_text\n",
    "\n",
    "# def pad_ds(example):\n",
    "#   padded_input = tf.keras.preprocessing.sequence.pad_sequences(.numpy(),padding='post',maxlen=MAX_LEN)\n",
    "#   return padded_input\n",
    "\n",
    "encoded_dataset = train_dataset.map(encode_ds_map_fn)\n",
    "# train_dataset = encoded_dataset.map(pad_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Aq1t0KZuBUY"
   },
   "outputs": [],
   "source": [
    "for item in encoded_dataset:\n",
    "  print(len(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnoPB5CNvkmy"
   },
   "source": [
    "Try wikipedia dataset from huggingface/nlp or tfds library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0HWzI5pvkKE"
   },
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/huggingface/nlp.git\n",
    "!pip install apache-beam\n",
    "%pip install -q apache_beam mwparserfromhell\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2BAj5a7wO0r"
   },
   "outputs": [],
   "source": [
    "huwiki_nlp = nlp.load_dataset('wikipedia','20200501.hu',beam_runner='DirectRunner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vW7f-fK9zC8H"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets -U\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBQYGjekzE7i"
   },
   "outputs": [],
   "source": [
    "huwiki = tfds.load('wikipedia/20200301.hu',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5B9AvsTzX7C"
   },
   "outputs": [],
   "source": [
    "huwiki_text = huwiki.map(lambda example: example['text']).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIXo-glWBQ_T"
   },
   "outputs": [],
   "source": [
    "def convert_to_tf_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    batch = list(example_batch)\n",
    "    encodings = gpt_tokenizer.batch_encode_plus(batch, max_length=512)\n",
    "\n",
    "    # # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methods.\n",
    "    # start_positions, end_positions = [], []\n",
    "    # for i, (context, answer) in enumerate(zip(example_batch['context'], example_batch['answers'])):\n",
    "    #     start_idx, end_idx = get_correct_alignement(context, answer)\n",
    "    #     start_positions.append([encodings.char_to_token(i, start_idx)])\n",
    "    #     end_positions.append([encodings.char_to_token(i, end_idx-1)])\n",
    "    \n",
    "    # if start_positions and end_positions:\n",
    "    #   encodings.update({'start_positions': start_positions,\n",
    "    #                     'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "huwiki_text_tokenized = huwiki_text.map(convert_to_tf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhwxZcEs1qRF"
   },
   "outputs": [],
   "source": [
    "tokens = gpt_tokenizer.encode(next(iter(huwiki_text.take(100))).numpy().decode('utf-8'))\n",
    "examples = []\n",
    "max_len=512\n",
    "for i in range(0, len(tokens) - max_len + 1, max_len):  # Truncate in block of block_size\n",
    "  examples.append(tokens[i : i + max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkLs8xMR0ORF"
   },
   "outputs": [],
   "source": [
    "def tokenize_example(example):\n",
    "    tokens = gpt_tokenizer.encode(next(iter(example.numpy().decode('utf-8'))))\n",
    "    max_len=512\n",
    "    for i in range(0, len(tokens) - max_len + 1, max_len):  # Truncate in block of block_size\n",
    "      examples.append(tokens[i : i + max_len])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yKzG3Cj2Fkn"
   },
   "outputs": [],
   "source": [
    "huwiki_text.map(tokenize_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Fafv35e0jF7",
    "outputId": "d6e5e4ba-f5a0-437c-b466-80365ee570e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gpt_tokenizer.encode(next(iter(huwiki_text.take(1))).numpy().decode('utf-8'),,return_tensors='tf'))\n",
    "overflowing_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-ZJWvxO1Pwn"
   },
   "outputs": [],
   "source": [
    "print(next(iter(huwiki_text.take(1))).numpy().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tokenization_for_TPU-Trainer_ba.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
